import os, tempfile, requests, logging, asyncio, re, hashlib
from datetime import datetime
from typing import List, Dict, Optional
from telegram import Update, InlineKeyboardButton, InlineKeyboardMarkup
from telegram.ext import ApplicationBuilder, MessageHandler, CommandHandler, ContextTypes, filters, ConversationHandler, CallbackQueryHandler

# ================== Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ù…Ø­Ø³Ù†Ø© ==================
MAX_FILE_SIZE = 50 * 1024 * 1024
MAX_LINES = 200000
CHUNK_SIZE = 5000

# ================== ØªÙˆÙƒÙ†Ø§Øª Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ ==================
BACKEND_KEYS = {
    "KIMI": {
        "key": "sk-your-kimi-token-here",
        "base": "https://api.moonshot.cn/v1",
        "models": ["moonshot-v1-8k", "moonshot-v1-32k", "moonshot-v1-128k"],
        "context": 128000
    },
    "DEEPSEEK": {
        "key": "sk-your-deepseek-token-here", 
        "base": "https://api.deepseek.com",
        "models": ["deepseek-chat", "deepseek-coder"],
        "context": 64000
    },
    "CHATGPT": {
        "key": "sk-your-openai-token-here",
        "base": "https://api.openai.com/v1",
        "models": ["gpt-4", "gpt-3.5-turbo"],
        "context": 128000
    },
    "CLAUDE": {
        "key": "sk-your-claude-token-here",
        "base": "https://api.anthropic.com/v1",
        "models": ["claude-3-opus-20240229", "claude-3-sonnet-20240229"],
        "context": 200000
    },
    "GEMINI": {
        "key": "sk-your-gemini-token-here",
        "base": "https://generativelanguage.googleapis.com/v1",
        "models": ["gemini-pro", "gemini-pro-vision"],
        "context": 32768
    },
    "QIANWEN": {
        "key": "sk-your-qianwen-token-here",
        "base": "https://dashscope.aliyuncs.com/api/v1",
        "models": ["qwen-turbo", "qwen-plus", "qwen-max"],
        "context": 128000
    },
    "ZHIPU": {
        "key": "sk-your-zhipu-token-here", 
        "base": "https://open.bigmodel.cn/api/paas/v4",
        "models": ["glm-4", "glm-3-turbo"],
        "context": 128000
    }
}

user_sessions = {}
file_history = {}

# ================== Ù†Ø¸Ø§Ù… Ø§Ù„Ø¯Ù…Ø¬ Ø§Ù„Ù…ØªÙ‚Ø¯Ù… ==================
class AdvancedMerger:
    def __init__(self):
        self.versions = []
        self.consensus_threshold = 0.7
    
    async def create_multiple_versions(self, filename: str, content: str, instructions: str, models: List[str]) -> List[Dict]:
        """Ø¥Ù†Ø´Ø§Ø¡ Ù†Ø³Ø® Ù…ØªØ¹Ø¯Ø¯Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù†Ù…Ø§Ø°Ø¬ Ù…Ø®ØªÙ„ÙØ©"""
        tasks = []
        for model_name in models:
            task = asyncio.create_task(
                self.generate_version(model_name, filename, content, instructions)
            )
            tasks.append(task)
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        valid_versions = []
        for i, result in enumerate(results):
            if isinstance(result, dict) and result.get("success"):
                valid_versions.append({
                    "model": models[i],
                    "content": result["content"],
                    "quality_score": self.assess_quality(result["content"], content),
                    "tokens_used": result.get("tokens_used", 0)
                })
        
        return valid_versions
    
    async def generate_version(self, model_name: str, filename: str, content: str, instructions: str) -> Dict:
        """Ø¥Ù†Ø´Ø§Ø¡ Ù†Ø³Ø®Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù†Ù…ÙˆØ°Ø¬ Ù…Ø­Ø¯Ø¯"""
        if model_name not in BACKEND_KEYS:
            return {"success": False, "error": f"Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ {model_name} ØºÙŠØ± Ù…Ø¯Ø¹ÙˆÙ…"}
        
        info = BACKEND_KEYS[model_name]
        
        if model_name == "CHATGPT":
            return await self.call_openai_api(info, filename, content, instructions)
        elif model_name == "CLAUDE":
            return await self.call_claude_api(info, filename, content, instructions)
        elif model_name == "GEMINI":
            return await self.call_gemini_api(info, filename, content, instructions)
        else:
            return await self.call_standard_api(info, filename, content, instructions, model_name)
    
    async def merge_versions(self, versions: List[Dict], original_content: str, instructions: str) -> Dict:
        """Ø¯Ù…Ø¬ Ø§Ù„Ù†Ø³Ø® Ø§Ù„Ù…ØªØ¹Ø¯Ø¯Ø© ÙˆØ§Ø®ØªÙŠØ§Ø± Ø§Ù„Ø£ÙØ¶Ù„"""
        if not versions:
            return {"success": False, "error": "Ù„Ø§ ØªÙˆØ¬Ø¯ Ù†Ø³Ø® Ù„Ù„Ø¯Ù…Ø¬"}
        
        if len(versions) == 1:
            return {"success": True, "content": versions[0]["content"], "method": "single"}
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªÙˆØ§ÙÙ‚ Ø¨ÙŠÙ† Ø§Ù„Ù†Ø³Ø®
        compatibility_scores = self.analyze_compatibility(versions)
        
        # Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ø£ÙØ¶Ù„ Ø¬ÙˆØ¯Ø©
        best_version = max(versions, key=lambda x: x["quality_score"])
        
        if compatibility_scores["consensus_achieved"]:
            # Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ù†Ø³Ø® Ù…ØªÙˆØ§ÙÙ‚Ø©ØŒ Ù†Ø¯Ù…Ø¬Ù‡Ø§
            merged_content = await self.smart_merge(versions, original_content, instructions)
            return {
                "success": True, 
                "content": merged_content,
                "method": "consensus_merge",
                "models_used": [v["model"] for v in versions],
                "compatibility_score": compatibility_scores["average"]
            }
        else:
            # Ø¥Ø°Ø§ Ù„Ù… ØªÙƒÙ† Ù…ØªÙˆØ§ÙÙ‚Ø©ØŒ Ù†Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ø£ÙØ¶Ù„
            return {
                "success": True,
                "content": best_version["content"],
                "method": "best_quality",
                "selected_model": best_version["model"],
                "quality_score": best_version["quality_score"]
            }
    
    def analyze_compatibility(self, versions: List[Dict]) -> Dict:
        """ØªØ­Ù„ÙŠÙ„ Ù…Ø¯Ù‰ ØªÙˆØ§ÙÙ‚ Ø§Ù„Ù†Ø³Ø® Ù…Ø¹ Ø¨Ø¹Ø¶Ù‡Ø§"""
        if len(versions) < 2:
            return {"consensus_achieved": True, "average": 1.0}
        
        scores = []
        for i in range(len(versions)):
            for j in range(i + 1, len(versions)):
                similarity = self.calculate_similarity(
                    versions[i]["content"], 
                    versions[j]["content"]
                )
                scores.append(similarity)
        
        avg_score = sum(scores) / len(scores) if scores else 0
        consensus = avg_score >= self.consensus_threshold
        
        return {
            "consensus_achieved": consensus,
            "average": avg_score,
            "min": min(scores) if scores else 0,
            "max": max(scores) if scores else 0
        }
    
    def calculate_similarity(self, text1: str, text2: str) -> float:
        """Ø­Ø³Ø§Ø¨ Ø¯Ø±Ø¬Ø© Ø§Ù„ØªØ´Ø§Ø¨Ù‡ Ø¨ÙŠÙ† Ù†ØµÙŠÙ†"""
        lines1 = set(text1.split('\n'))
        lines2 = set(text2.split('\n'))
        
        if not lines1 and not lines2:
            return 1.0
        
        intersection = len(lines1.intersection(lines2))
        union = len(lines1.union(lines2))
        
        return intersection / union if union > 0 else 0
    
    def assess_quality(self, new_content: str, original_content: str) -> float:
        """ØªÙ‚ÙŠÙŠÙ… Ø¬ÙˆØ¯Ø© Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø¬Ø¯ÙŠØ¯"""
        original_lines = original_content.split('\n')
        new_lines = new_content.split('\n')
        
        if not original_lines or not new_lines:
            return 0.5
        
        # Ø­Ø³Ø§Ø¨ Ù†Ø³Ø¨Ø© Ø§Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ø§Ù„Ù‡ÙŠÙƒÙ„
        structure_preservation = len(new_lines) / max(len(original_lines), 1)
        structure_preservation = min(structure_preservation, 2.0) / 2.0  # ØªØ·Ø¨ÙŠØ¹
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªØ¹Ù‚ÙŠØ¯ (Ù†ÙØ¶Ù„ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù…Ø¹Ù‚Ø¯ Ù‚Ù„ÙŠÙ„Ø§Ù‹)
        original_complexity = self.calculate_complexity(original_content)
        new_complexity = self.calculate_complexity(new_content)
        complexity_score = 0.5 + (new_complexity - original_complexity) * 0.5
        
        # Ø§Ù„Ø¯Ø±Ø¬Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©
        final_score = (structure_preservation * 0.6 + complexity_score * 0.4)
        return max(0.1, min(1.0, final_score))
    
    def calculate_complexity(self, text: str) -> float:
        """Ø­Ø³Ø§Ø¨ ØªØ¹Ù‚ÙŠØ¯ Ø§Ù„Ù†Øµ"""
        lines = text.split('\n')
        if not lines:
            return 0
        
        code_lines = [line for line in lines if line.strip() and not line.strip().startswith('#')]
        avg_line_length = sum(len(line) for line in code_lines) / max(len(code_lines), 1)
        
        return min(avg_line_length / 100, 1.0)
    
    async def smart_merge(self, versions: List[Dict], original: str, instructions: str) -> str:
        """Ø¯Ù…Ø¬ Ø°ÙƒÙŠ Ù„Ù„Ù†Ø³Ø® Ø§Ù„Ù…ØªØ¹Ø¯Ø¯Ø©"""
        if len(versions) == 1:
            return versions[0]["content"]
        
        # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£ÙØ¶Ù„ Ù†Ù…ÙˆØ°Ø¬ Ù„Ø¯Ù…Ø¬ Ø§Ù„Ù†Ø³Ø®
        merger_model = "CHATGPT"  # ÙŠÙ…ÙƒÙ† ØªØºÙŠÙŠØ±Ù‡ Ù„Ø£ÙŠ Ù†Ù…ÙˆØ°Ø¬ Ø¢Ø®Ø±
        
        merger_prompt = f"""Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ø®Ø¨ÙŠØ± ÙÙŠ Ø¯Ù…Ø¬ Ø§Ù„Ø´ÙŠÙØ±Ø§Øª Ø§Ù„Ø¨Ø±Ù…Ø¬ÙŠØ©. Ù„Ø¯ÙŠÙƒ {len(versions)} Ù†Ø³Ø® Ù…Ø®ØªÙ„ÙØ© Ù…Ù† Ù†ÙØ³ Ø§Ù„Ù…Ù„ÙØŒ ÙƒÙ„ Ù…Ù†Ù‡Ø§ Ø£Ù†Ø´Ø¦ Ø¨ÙˆØ§Ø³Ø·Ø© Ù†Ù…ÙˆØ°Ø¬ Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ù…Ø®ØªÙ„Ù.

Ø§Ù„ØªØ¹Ù„ÙŠÙ…Ø§Øª Ø§Ù„Ø£ØµÙ„ÙŠØ©: {instructions}

Ø§Ù„Ù…Ø·Ù„ÙˆØ¨:
1. Ù‚Ø§Ø±Ù† Ø¨ÙŠÙ† Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù†Ø³Ø®
2. Ø§Ø®ØªØ± Ø£ÙØ¶Ù„ Ø§Ù„Ø£Ø¬Ø²Ø§Ø¡ Ù…Ù† ÙƒÙ„ Ù†Ø³Ø®Ø©
3. Ø§Ø¯Ù…Ø¬Ù‡Ø§ ÙÙŠ Ù†Ø³Ø®Ø© ÙˆØ§Ø­Ø¯Ø© Ù…ØªÙ…Ø§Ø³ÙƒØ©
4. ØªØ£ÙƒØ¯ Ù…Ù†:
   - Ø¹Ø¯Ù… ÙˆØ¬ÙˆØ¯ ØªØ¹Ø§Ø±Ø¶Ø§Øª Ø¨ÙŠÙ† Ø§Ù„Ø£Ø¬Ø²Ø§Ø¡
   - Ø§Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ø§Ù„ØªÙ†Ø§Ø³Ù‚ ÙÙŠ Ø§Ù„Ù†Ù…Ø·
   - ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„ØªØ¹Ù„ÙŠÙ…Ø§Øª Ø¨Ø¯Ù‚Ø©
   - ØµØ­Ø© Ø§Ù„Ø´ÙŠÙØ±Ø© Ù…Ù† Ø§Ù„Ù†Ø§Ø­ÙŠØ© Ø§Ù„ÙˆØ¸ÙŠÙÙŠØ©

Ø£Ø¹Ø¯ Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ù…Ø¯Ù…Ø¬Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ© ÙÙ‚Ø·:"""
        
        comparison_text = "\n\n".join([
            f"=== Ù†Ø³Ø®Ø© {v['model']} (Ø¬ÙˆØ¯Ø©: {v['quality_score']:.2f}) ===\n{v['content']}" 
            for v in versions
        ])
        
        full_prompt = f"{merger_prompt}\n\n{comparison_text}"
        
        # Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯ØºÙ…
        result = await self.call_standard_api(
            BACKEND_KEYS[merger_model], 
            "merged_version", 
            original, 
            full_prompt, 
            merger_model
        )
        
        if result.get("success"):
            return result["content"]
        else:
            # Ø¥Ø°Ø§ ÙØ´Ù„ Ø§Ù„Ø¯Ù…Ø¬ØŒ Ù†Ø¹ÙˆØ¯ Ø¨Ø£ÙØ¶Ù„ Ù†Ø³Ø®Ø©
            best_version = max(versions, key=lambda x: x["quality_score"])
            return best_version["content"]
    
    async def call_standard_api(self, info: Dict, filename: str, content: str, instructions: str, model_name: str) -> Dict:
        """Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ API Ù‚ÙŠØ§Ø³ÙŠ"""
        url = f"{info['base']}/chat/completions"
        
        system_prompt = f"""Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ø¨Ø±Ù…Ø¬ÙŠ Ø®Ø¨ÙŠØ±. Ø§Ù„Ù…Ù„Ù: {filename}

Ø§Ù„ØªØ¹Ù„ÙŠÙ…Ø§Øª: {instructions}

Ø§Ù„Ù…Ø·Ù„ÙˆØ¨:
1. ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„ØªØ¹Ù„ÙŠÙ…Ø§Øª Ø¨Ø¯Ù‚Ø© Ø¹Ø§Ù„ÙŠØ©
2. Ø§Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ù†Ù…Ø· Ø§Ù„Ø¨Ø±Ù…Ø¬Ø© Ø§Ù„Ø£ØµÙ„ÙŠ  
3. Ø¥Ø¶Ø§ÙØ© ØªØ¹Ù„ÙŠÙ‚Ø§Øª ØªÙˆØ¶ÙŠØ­ÙŠØ© Ø¹Ù†Ø¯ Ø§Ù„Ø­Ø§Ø¬Ø©
4. Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† ØµØ­Ø© Ø§Ù„Ø´ÙŠÙØ±Ø©

Ø£Ø¹Ø¯ Ø§Ù„Ø´ÙŠÙØ±Ø© Ø§Ù„Ù…Ø­Ø³Ù†Ø© ÙÙ‚Ø·:"""
        
        payload = {
            "model": info["models"][0],
            "messages": [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": f"```\n{content}\n```"}
            ],
            "temperature": 0.1,
            "max_tokens": min(info["context"] - 1000, 32000)
        }
        
        headers = {
            "Authorization": f"Bearer {info['key']}",
            "Content-Type": "application/json"
        }
        
        try:
            async with asyncio.timeout(60):
                response = requests.post(url, headers=headers, json=payload, timeout=60)
                response.raise_for_status()
                result = response.json()
                
                content = result.get("choices", [{}])[0].get("message", {}).get("content", "")
                
                # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø´ÙŠÙØ±Ø© Ù…Ù† code blocks
                code_blocks = re.findall(r'```(?:\w+)?\n(.*?)\n```', content, re.DOTALL)
                if code_blocks:
                    content = code_blocks[0]
                else:
                    content = content.strip()
                
                return {
                    "success": True,
                    "content": content,
                    "tokens_used": result.get("usage", {}).get("total_tokens", 0)
                }
        except Exception as e:
            return {"success": False, "error": str(e)}
    
    async def call_openai_api(self, info: Dict, filename: str, content: str, instructions: str) -> Dict:
        """Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ OpenAI API"""
        return await self.call_standard_api(info, filename, content, instructions, "CHATGPT")
    
    async def call_claude_api(self, info: Dict, filename: str, content: str, instructions: str) -> Dict:
        """Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ Claude API"""
        url = f"{info['base']}/messages"
        
        system_prompt = f"""Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ø¨Ø±Ù…Ø¬ÙŠ Ø®Ø¨ÙŠØ±. Ø§Ù„Ù…Ù„Ù: {filename}

Ø§Ù„ØªØ¹Ù„ÙŠÙ…Ø§Øª: {instructions}

Ø£Ø¹Ø¯ Ø§Ù„Ø´ÙŠÙØ±Ø© Ø§Ù„Ù…Ø­Ø³Ù†Ø© ÙÙ‚Ø·:"""
        
        payload = {
            "model": info["models"][0],
            "max_tokens": 4000,
            "system": system_prompt,
            "messages": [
                {
                    "role": "user",
                    "content": f"```\n{content}\n```"
                }
            ]
        }
        
        headers = {
            "x-api-key": info["key"],
            "Content-Type": "application/json",
            "anthropic-version": "2023-06-01"
        }
        
        try:
            async with asyncio.timeout(60):
                response = requests.post(url, headers=headers, json=payload, timeout=60)
                response.raise_for_status()
                result = response.json()
                
                content = result.get("content", [{}])[0].get("text", "").strip()
                
                return {
                    "success": True,
                    "content": content,
                    "tokens_used": result.get("usage", {}).get("input_tokens", 0)
                }
        except Exception as e:
            return {"success": False, "error": str(e)}
    
    async def call_gemini_api(self, info: Dict, filename: str, content: str, instructions: str) -> Dict:
        """Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ Gemini API"""
        url = f"{info['base']}/models/{info['models'][0]}:generateContent?key={info['key']}"
        
        prompt = f"""Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ø¨Ø±Ù…Ø¬ÙŠ Ø®Ø¨ÙŠØ±. 

Ø§Ù„Ù…Ù„Ù: {filename}
Ø§Ù„ØªØ¹Ù„ÙŠÙ…Ø§Øª: {instructions}

Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø£ØµÙ„ÙŠ:

Ø£Ø¹Ø¯ Ø§Ù„Ø´ÙŠÙØ±Ø© Ø§Ù„Ù…Ø­Ø³Ù†Ø© ÙÙ‚Ø·:"""
        
        payload = {
            "contents": [{
                "parts": [{"text": prompt}]
            }],
            "generationConfig": {
                "temperature": 0.1,
                "maxOutputTokens": 4000
            }
        }
        
        headers = {
            "Content-Type": "application/json"
        }
        
        try:
            async with asyncio.timeout(60):
                response = requests.post(url, headers=headers, json=payload, timeout=60)
                response.raise_for_status()
                result = response.json()
                
                content = result.get("candidates", [{}])[0].get("content", {}).get("parts", [{}])[0].get("text", "").strip()
                
                return {
                    "success": True,
                    "content": content
                }
        except Exception as e:
            return {"success": False, "error": str(e)}

# ================== ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù…Ø­Ø³Ù†Ø© ==================
async def start_enhanced(update: Update, ctx: ContextTypes.DEFAULT_TYPE):
    """Ø¨Ø¯Ø¡ Ù…Ø­Ø§Ø¯Ø«Ø© Ù…Ø¹ Ù†Ø¸Ø§Ù… Ø§Ù„Ø¯Ù…Ø¬ Ø§Ù„Ù…ØªØ¹Ø¯Ø¯"""
    user = update.effective_user
    
    # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…ØªØ§Ø­Ø©
    available_backends = validate_backend_keys()
    active_backends = [name for name, valid in available_backends.items() if valid]
    
    if not active_backends:
        await update.message.reply_text(
            "âŒ **Ù„Ø§ ØªÙˆØ¬Ø¯ Ù†Ù…Ø§Ø°Ø¬ Ù…ØªØ§Ø­Ø©**\n\nÙŠØ±Ø¬Ù‰ Ø¶Ø¨Ø· Ø§Ù„ØªÙˆÙƒÙ†Ø§Øª Ø£ÙˆÙ„Ø§Ù‹.",
            parse_mode='Markdown'
        )
        return ConversationHandler.END
    
    user_id = user.id
    user_sessions[user_id] = {
        "selected_models": [],
        "merger": AdvancedMerger()
    }
    
    # Ø¥Ù†Ø´Ø§Ø¡ ÙˆØ§Ø¬Ù‡Ø© Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ù†Ù…Ø§Ø°Ø¬
    keyboard = []
    for backend in active_backends:
        emoji = "âœ…" if available_backends[backend] else "âŒ"
        keyboard.append([InlineKeyboardButton(f"{emoji} {backend}", callback_data=f"model_{backend}")])
    
    # Ø¥Ø¶Ø§ÙØ© Ø®ÙŠØ§Ø±Ø§Øª Ø§Ù„Ø¯Ù…Ø¬
    keyboard.append([InlineKeyboardButton("ğŸš€ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ (Ø¯Ù…Ø¬ ØªÙ„Ù‚Ø§Ø¦ÙŠ)", callback_data="model_all")])
    keyboard.append([InlineKeyboardButton("ğŸ”€ Ø¯Ù…Ø¬ Ù…Ø®ØµØµ", callback_data="model_custom")])
    keyboard.append([InlineKeyboardButton("âœ… ØªØ£ÙƒÙŠØ¯ Ø§Ù„Ø§Ø®ØªÙŠØ§Ø±", callback_data="model_confirm")])
    
    reply_markup = InlineKeyboardMarkup(keyboard)
    
    await update.message.reply_text(
        f"ğŸ› ï¸ **Ù…Ø±Ø­Ø¨Ø§Ù‹ {user.first_name}!**\n\n"
        f"ğŸ“Š **Ø§Ø®ØªØ± Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ:**\n"
        f"({len(active_backends)} Ù†Ù…ÙˆØ°Ø¬ Ù…ØªØ§Ø­)\n\n"
        f"ğŸ’¡ **Ù…ÙŠØ²Ø© Ø§Ù„Ø¯Ù…Ø¬ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…:**\n"
        f"â€¢ Ø¥Ù†Ø´Ø§Ø¡ Ù†Ø³Ø® Ù…ØªØ¹Ø¯Ø¯Ø©\nâ€¢ Ù…Ù‚Ø§Ø±Ù†Ø© ØªÙ„Ù‚Ø§Ø¦ÙŠØ©\nâ€¢ Ø¯Ù…Ø¬ Ø°ÙƒÙŠ\nâ€¢ Ø§Ø®ØªÙŠØ§Ø± Ø£ÙØ¶Ù„ Ù†Ø³Ø®Ø©",
        reply_markup=reply_markup,
        parse_mode='Markdown'
    )
    
    return WAITING_BACKEND_CHOICE

async def handle_model_selection(update: Update, ctx: ContextTypes.DEFAULT_TYPE):
    """Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ù†Ù…Ø§Ø°Ø¬"""
    query = update.callback_query
    user_id = query.from_user.id
    data = query.data
    
    await query.answer()
    
    if user_id not in user_sessions:
        user_sessions[user_id] = {"selected_models": [], "merger": AdvancedMerger()}
    
    if data == "model_all":
        # Ø§Ø®ØªÙŠØ§Ø± Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…ØªØ§Ø­Ø©
        available_backends = validate_backend_keys()
        user_sessions[user_id]["selected_models"] = [
            name for name, valid in available_backends.items() if valid
        ]
        
        await query.edit_message_text(
            f"âœ… **ØªÙ… Ø§Ø®ØªÙŠØ§Ø± Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ ({len(user_sessions[user_id]['selected_models'])})**\n\n"
            f"ğŸ“ **Ø§Ù„Ø¢Ù† Ø£Ø±Ø³Ù„ Ø§Ù„Ù…Ù„Ù ÙƒÙ€ Document**\n\n"
            f"ğŸ”§ Ø³Ø£Ù‚ÙˆÙ… Ø¨:\n"
            f"â€¢ Ø¥Ù†Ø´Ø§Ø¡ Ù†Ø³Ø®Ø© Ù…Ù† ÙƒÙ„ Ù†Ù…ÙˆØ°Ø¬\nâ€¢ Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ù†ØªØ§Ø¦Ø¬\nâ€¢ Ø¯Ù…Ø¬Ù‡Ø§ ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹\nâ€¢ Ø¥Ø±Ø¬Ø§Ø¹ Ø£ÙØ¶Ù„ Ù†Ø³Ø®Ø©",
            parse_mode='Markdown'
        )
        return WAITING_FILE
    
    elif data == "model_custom":
        # Ø§Ø®ØªÙŠØ§Ø± Ù…Ø®ØµØµ
        await query.edit_message_text(
            "ğŸ”€ **Ø§Ù„Ø¯Ù…Ø¬ Ø§Ù„Ù…Ø®ØµØµ**\n\n"
            "Ø§ÙƒØªØ±Ø¨ Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ù…ÙØµÙˆÙ„Ø© Ø¨ÙØ§ØµÙ„Ø©:\n"
            "Ù…Ø«Ø§Ù„: `CHATGPT, DEEPSEEK, CLAUDE`\n\n"
            "Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…ØªØ§Ø­Ø©: " + ", ".join(BACKEND_KEYS.keys()),
            parse_mode='Markdown'
        )
        return WAITING_CUSTOM_MODELS
    
    elif data == "model_confirm":
        if not user_sessions[user_id]["selected_models"]:
            await query.edit_message_text(
                "âŒ Ù„Ù… ØªØ®ØªØ± Ø£ÙŠ Ù†Ù…Ø§Ø°Ø¬. Ø§Ø®ØªØ± Ù†Ù…Ø§Ø°Ø¬ Ø£ÙˆÙ„Ø§Ù‹.",
                parse_mode='Markdown'
            )
            return WAITING_BACKEND_CHOICE
        
        await query.edit_message_text(
            f"âœ… **ØªÙ… Ø§Ø®ØªÙŠØ§Ø± {len(user_sessions[user_id]['selected_models'])} Ù†Ù…Ø§Ø°Ø¬**\n\n"
            f"ğŸ“ **Ø§Ù„Ø¢Ù† Ø£Ø±Ø³Ù„ Ø§Ù„Ù…Ù„Ù ÙƒÙ€ Document**\n\n"
            f"Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…Ø®ØªØ§Ø±Ø©: {', '.join(user_sessions[user_id]['selected_models'])}",
            parse_mode='Markdown'
        )
        return WAITING_FILE
    
    else:
        # Ø§Ø®ØªÙŠØ§Ø± Ù†Ù…ÙˆØ°Ø¬ ÙØ±Ø¯ÙŠ
        model_name = data.replace("model_", "")
        if model_name in user_sessions[user_id]["selected_models"]:
            user_sessions[user_id]["selected_models"].remove(model_name)
            await query.answer(f"ØªÙ… Ø¥Ø²Ø§Ù„Ø© {model_name}")
        else:
            user_sessions[user_id]["selected_models"].append(model_name)
            await query.answer(f"ØªÙ… Ø¥Ø¶Ø§ÙØ© {model_name}")
        
        # ØªØ­Ø¯ÙŠØ« Ø§Ù„ÙˆØ§Ø¬Ù‡Ø©
        available_backends = validate_backend_keys()
        keyboard = []
        for backend in available_backends:
            emoji = "ğŸŸ¢" if backend in user_sessions[user_id]["selected_models"] else "âšª"
            status = "âœ…" if available_backends[backend] else "âŒ"
            keyboard.append([InlineKeyboardButton(f"{emoji} {status} {backend}", callback_data=f"model_{backend}")])
        
        keyboard.append([InlineKeyboardButton("ğŸš€ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬", callback_data="model_all")])
        keyboard.append([InlineKeyboardButton("ğŸ”€ Ø¯Ù…Ø¬ Ù…Ø®ØµØµ", callback_data="model_custom")])
        keyboard.append([InlineKeyboardButton(f"âœ… ØªØ£ÙƒÙŠØ¯ ({len(user_sessions[user_id]['selected_models'])})", callback_data="model_confirm")])
        
        reply_markup = InlineKeyboardMarkup(keyboard)
        
        await query.edit_message_text(
            f"ğŸ”§ **Ø§Ø®ØªØ± Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ ({len(user_sessions[user_id]['selected_models'])} Ù…Ø®ØªØ§Ø±Ø©)**\n\n"
            f"Ø§Ù„Ù…Ø®ØªØ§Ø±Ø©: {', '.join(user_sessions[user_id]['selected_models']) or 'Ù„Ø§ Ø´ÙŠØ¡'}",
            reply_markup=reply_markup,
            parse_mode='Markdown'
        )
        return WAITING_BACKEND_CHOICE

async def handle_custom_models(update: Update, ctx: ContextTypes.DEFAULT_TYPE):
    """Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ù…Ø®ØµØµ Ù„Ù„Ù†Ù…Ø§Ø°Ø¬"""
    user_id = update.effective_user.id
    text = update.message.text
    
    if user_id not in user_sessions:
        user_sessions[user_id] = {"selected_models": [], "merger": AdvancedMerger()}
    
    # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…Ø¯Ø®Ù„Ø©
    input_models = [model.strip().upper() for model in text.split(',')]
    valid_models = []
    
    for model in input_models:
        if model in BACKEND_KEYS:
            valid_models.append(model)
        else:
            await update.message.reply_text(f"âŒ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ {model} ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙ")
    
    if not valid_models:
        await update.message.reply_text("âŒ Ù„Ù… ØªØ¯Ø®Ù„ Ø£ÙŠ Ù†Ù…Ø§Ø°Ø¬ ØµØ­ÙŠØ­Ø©")
        return WAITING_BACKEND_CHOICE
    
    user_sessions[user_id]["selected_models"] = valid_models
    
    await update.message.reply_text(
        f"âœ… **ØªÙ… Ø§Ø®ØªÙŠØ§Ø± {len(valid_models)} Ù†Ù…Ø§Ø°Ø¬**\n\n"
        f"ğŸ“ **Ø§Ù„Ø¢Ù† Ø£Ø±Ø³Ù„ Ø§Ù„Ù…Ù„Ù ÙƒÙ€ Document**\n\n"
        f"Ø§Ù„Ù†Ù…Ø§Ø°Ø¬: {', '.join(valid_models)}",
        parse_mode='Markdown'
    )
    return WAITING_FILE

# ================== Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ù„Ù Ù…Ø¹ Ø§Ù„Ø¯Ù…Ø¬ Ø§Ù„Ù…ØªÙ‚Ø¯Ù… ==================
async def process_file_with_merging(update: Update, ctx: ContextTypes.DEFAULT_TYPE):
    """Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ù„Ù Ù…Ø¹ Ù†Ø¸Ø§Ù… Ø§Ù„Ø¯Ù…Ø¬ Ø§Ù„Ù…ØªØ¹Ø¯Ø¯"""
    user_id = update.effective_user.id
    
    if user_id not in user_sessions or not user_sessions[user_id].get("selected_models"):
        await update.message.reply_text("âŒ Ù„Ù… ØªØ®ØªØ± Ø£ÙŠ Ù†Ù…Ø§Ø°Ø¬. Ø§Ø¨Ø¯Ø£ Ø¨Ù€ /start")
        return ConversationHandler.END
    
    if not update.message.document:
        await update.message.reply_text("âŒ ÙŠØ±Ø¬Ù‰ Ø¥Ø±Ø³Ø§Ù„ Ù…Ù„Ù")
        return WAITING_FILE
    
    # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ù„Ù
    file = await ctx.bot.get_file(update.message.document.file_id)
    filename = update.message.document.file_name or "unknown"
    
    progress_msg = await update.message.reply_text("ğŸ“¥ **Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ù„Ù...**")
    
    try:
        with tempfile.TemporaryDirectory() as tmp:
            path = os.path.join(tmp, sanitize_filename(filename))
            await file.download_to_drive(custom_path=path)
            
            with open(path, "r", encoding="utf-8", errors="ignore") as f:
                content = f.read()
        
        await progress_msg.edit_text("ğŸ”„ **Ø¬Ø§Ø±ÙŠ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Ø³Ø® Ø§Ù„Ù…ØªØ¹Ø¯Ø¯Ø©...**")
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ù†Ø³Ø® Ù…ØªØ¹Ø¯Ø¯Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…Ø®ØªØ§Ø±Ø©
        merger = user_sessions[user_id]["merger"]
        models = user_sessions[user_id]["selected_models"]
        
        versions = await merger.create_multiple_versions(
            filename, content, "ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø´ÙŠÙØ±Ø©", models
        )
        
        if not versions:
            await progress_msg.edit_text("âŒ ÙØ´Ù„Øª Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ù„Ù")
            return ConversationHandler.END
        
        await progress_msg.edit_text(f"ğŸ”€ **Ø¬Ø§Ø±ÙŠ Ø¯Ù…Ø¬ {len(versions)} Ù†Ø³Ø®Ø©...**")
        
        # Ø¯Ù…Ø¬ Ø§Ù„Ù†Ø³Ø®
        merge_result = await merger.merge_versions(versions, content, "ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø´ÙŠÙØ±Ø©")
        
        if merge_result["success"]:
            # Ø­ÙØ¸ Ø§Ù„Ù†ØªÙŠØ¬Ø©
            backup_path = create_backup(user_id, f"merged_{filename}", merge_result["content"])
            
            # Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ù†ØªÙŠØ¬Ø©
            result_text = f"""
ğŸ‰ **ØªÙ… Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨Ù†Ø¬Ø§Ø­!**

ğŸ“Š **Ø§Ù„Ù†ØªÙŠØ¬Ø©:**
â€¢ Ø§Ù„Ù…Ù„Ù: `{filename}`
â€¢ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø©: {len(versions)}
â€¢ Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„Ø¯Ù…Ø¬: {merge_result['method']}
â€¢ Ø¬ÙˆØ¯Ø© Ø§Ù„Ø¯Ù…Ø¬: {merge_result.get('compatibility_score', 'N/A')}

ğŸ’¾ **Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ù…Ø­Ø³Ù†Ø©:**"""
            
            await progress_msg.edit_text(result_text, parse_mode='Markdown')
            
            # Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù…Ø­Ø³Ù†
            with tempfile.NamedTemporaryFile(mode='w', suffix=filename, encoding='utf-8', delete=False) as f:
                f.write(merge_result["content"])
                temp_path = f.name
            
            try:
                with open(temp_path, 'rb') as file_obj:
                    await update.message.reply_document(
                        document=file_obj,
                        filename=f"enhanced_{filename}",
                        caption=f"ğŸ› ï¸ {filename} - Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ù…Ø¯Ù…Ø¬Ø© ({len(versions)} Ù†Ù…ÙˆØ°Ø¬)"
                    )
            finally:
                os.unlink(temp_path)
            
            # Ø¥Ø±Ø³Ø§Ù„ ØªÙØ§ØµÙŠÙ„ Ø¥Ø¶Ø§ÙÙŠØ©
            details_text = f"""
ğŸ“ˆ **ØªÙØ§ØµÙŠÙ„ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©:**
"""
            for version in versions:
                details_text += f"â€¢ {version['model']}: Ø¬ÙˆØ¯Ø© {version['quality_score']:.2f}\n"
            
            await update.message.reply_text(details_text, parse_mode='Markdown')
            
        else:
            await progress_msg.edit_text(f"âŒ ÙØ´Ù„ Ø§Ù„Ø¯Ù…Ø¬: {merge_result.get('error', 'Ø®Ø·Ø£ ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙ')}")
        
    except Exception as e:
        logging.error(f"Processing error: {e}")
        await progress_msg.edit_text(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©: {str(e)}")
    
    return ConversationHandler.END

# ================== Ø¯ÙˆØ§Ù„ Ù…Ø³Ø§Ø¹Ø¯Ø© Ù…ÙˆØ¬ÙˆØ¯Ø© Ø³Ø§Ø¨Ù‚Ø§Ù‹ ==================
def validate_token(token: str) -> bool:
    pattern = r'^\d+:[A-Za-z0-9_-]+$'
    return bool(re.match(pattern, token))

def sanitize_filename(filename: str) -> str:
    return re.sub(r'[^\w\-_.]', '_', filename)

def detect_language(filename: str) -> str:
    ext_lang_map = {
        '.py': 'Python', '.js': 'JavaScript', '.java': 'Java',
        '.cpp': 'C++', '.c': 'C', '.html': 'HTML', '.css': 'CSS',
        '.php': 'PHP', '.rb': 'Ruby', '.go': 'Go', '.rs': 'Rust',
        '.ts': 'TypeScript', '.sql': 'SQL', '.json': 'JSON',
        '.xml': 'XML', '.csv': 'CSV', '.txt': 'Text'
    }
    ext = os.path.splitext(filename)[1].lower()
    return ext_lang_map.get(ext, 'Ù†Øµ Ø¹Ø§Ø¯ÙŠ')

def create_backup(user_id: int, filename: str, content: str) -> str:
    backup_dir = f"backups/{user_id}"
    os.makedirs(backup_dir, exist_ok=True)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    safe_filename = sanitize_filename(filename)
    backup_file = f"{backup_dir}/{safe_filename}_{timestamp}.bak"
    
    try:
        with open(backup_file, 'w', encoding='utf-8') as f:
            f.write(content)
        return backup_file
    except Exception as e:
        logging.error(f"Backup failed: {e}")
        return ""

def validate_backend_keys() -> Dict[str, bool]:
    validation_results = {}
    
    for name, config in BACKEND_KEYS.items():
        if config["key"].startswith("sk-your-"):
            validation_results[name] = False
        else:
            validation_results[name] = True
    
    return validation_results

# ================== Ø§Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ ==================
def main():
    """ØªØ´ØºÙŠÙ„ Ø§Ù„Ø¨ÙˆØª Ù…Ø¹ Ù†Ø¸Ø§Ù… Ø§Ù„Ø¯Ù…Ø¬ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…"""
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù‡ÙŠÙƒÙ„ Ø§Ù„ØªÙ†Ø¸ÙŠÙ…ÙŠ
    os.makedirs("backups", exist_ok=True)
    os.makedirs("temp", exist_ok=True)
    os.makedirs("logs", exist_ok=True)
    
    # ØªÙˆÙƒÙ† Ø§Ù„Ø¨ÙˆØª
    BOT_TOKEN = "8214334664:AAGWEhTYrFTyN_TCxbFlQfdnIKYLgfI496A"
    
    # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…ØªØ§Ø­Ø©
    available_models = validate_backend_keys()
    active_models = [name for name, valid in available_models.items() if valid]
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler('logs/advanced_bot.log'),
            logging.StreamHandler()
        ]
    )
    
    logger = logging.getLogger(__name__)
    
    if not active_models:
        logger.warning("âš ï¸  Ù„Ø§ ØªÙˆØ¬Ø¯ Ù†Ù…Ø§Ø°Ø¬ Ù…Ø¶Ø¨ÙˆØ·Ø©!")
    else:
        logger.info(f"ğŸš€ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ø¬Ø§Ù‡Ø²Ø©: {', '.join(active_models)}")
    
    app = ApplicationBuilder()\
        .token(BOT_TOKEN)\
        .concurrent_updates(True)\
        .pool_timeout(120)\
        .read_timeout(120)\
        .write_timeout(120)\
        .build()

    # Ù…Ø¹Ø§Ù„Ø¬ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© Ø§Ù„Ù…Ø­Ø³Ù†
    conv_handler = ConversationHandler(
        entry_points=[CommandHandler("start", start_enhanced)],
        states={
            WAITING_BACKEND_CHOICE: [CallbackQueryHandler(handle_model_selection, pattern="^model_")],
            WAITING_CUSTOM_MODELS: [MessageHandler(filters.TEXT & ~filters.COMMAND, handle_custom_models)],
            WAITING_FILE: [MessageHandler(filters.Document.ALL, process_file_with_merging)]
        },
        fallbacks=[CommandHandler("cancel", cancel)],
        name="advanced_conv",
        allow_reentry=True
    )

    app.add_handler(conv_handler)
    app.add_handler(CommandHandler("setup", setup_command))
    app.add_handler(CommandHandler("help", help_enhanced))

    logger.info("ğŸš€ Ø¨Ø¯Ø¡ ØªØ´ØºÙŠÙ„ Ø§Ù„Ø¨ÙˆØª Ø§Ù„Ù…ØªÙ‚Ø¯Ù… Ù…Ø¹ Ù†Ø¸Ø§Ù… Ø§Ù„Ø¯Ù…Ø¬...")
    
    try:
        app.run_polling()
    except Exception as e:
        logger.error(f"âŒ ÙØ´Ù„ ØªØ´ØºÙŠÙ„ Ø§Ù„Ø¨ÙˆØª: {e}")
        raise

# Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø¯ÙˆØ§Ù„ Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø©
async def cancel(update: Update, ctx: ContextTypes.DEFAULT_TYPE):
    await update.message.reply_text("ØªÙ… Ø§Ù„Ø¥Ù„ØºØ§Ø¡")
    return ConversationHandler.END

async def setup_command(update: Update, ctx: ContextTypes.DEFAULT_TYPE):
    validation = validate_backend_keys()
    setup_text = "âš™ï¸ **Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù†Ù…Ø§Ø°Ø¬:**\n\n"
    
    for name, config in BACKEND_KEYS.items():
        status = "âœ… Ø¬Ø§Ù‡Ø²" if validation[name] else "âŒ ÙŠØ­ØªØ§Ø¬ Ø¥Ø¹Ø¯Ø§Ø¯"
        models = ", ".join(config["models"])
        setup_text += f"**{name}:** {status}\n"
        setup_text += f"   â€¢ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬: {models}\n\n"
    
    await update.message.reply_text(setup_text, parse_mode='Markdown')

async def help_enhanced(update: Update, ctx: ContextTypes.DEFAULT_TYPE):
    help_text = """
ğŸ› ï¸ **Ø¨ÙˆØª Ø§Ù„Ø¯Ù…Ø¬ Ø§Ù„Ù…ØªÙ‚Ø¯Ù… - Ø§Ù„ØªØ¹Ù„ÙŠÙ…Ø§Øª**

ğŸ¯ **Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©:**
â€¢ Ø¯Ø¹Ù… 7+ Ù†Ù…Ø§Ø°Ø¬ Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ
â€¢ Ù†Ø¸Ø§Ù… Ø¯Ù…Ø¬ Ø°ÙƒÙŠ Ù„Ù„Ù†Ø³Ø® Ø§Ù„Ù…ØªØ¹Ø¯Ø¯Ø©
â€¢ Ù…Ù‚Ø§Ø±Ù†Ø© ØªÙ„Ù‚Ø§Ø¦ÙŠØ© Ø¨ÙŠÙ† Ø§Ù„Ù†ØªØ§Ø¦Ø¬
â€¢ Ø§Ø®ØªÙŠØ§Ø± Ø£ÙØ¶Ù„ Ù†Ø³Ø®Ø© ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹

ğŸ“ **Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…:**
1. /start - Ø¨Ø¯Ø¡ Ø¬Ù„Ø³Ø© Ø¬Ø¯ÙŠØ¯Ø©
2. Ø§Ø®ØªØ± Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©
3. Ø£Ø±Ø³Ù„ Ø§Ù„Ù…Ù„Ù ÙƒÙ€ Document
4. Ø§Ø³ØªÙ„Ù… Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ù…Ø¯Ù…Ø¬Ø© Ø§Ù„Ù…Ø­Ø³Ù†Ø©

ğŸ”§ **Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…Ø¯Ø¹ÙˆÙ…Ø©:**
ChatGPT, Claude, Gemini, DeepSeek, Kimi, Qianwen, Zhipu
    """
    await update.message.reply_text(help_text, parse_mode='Markdown')

if __name__ == "__main__":
    main()
